<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Research Paper Analysis: Poor Alignment and Steerability of Large Language Models: Evidence from College Admission Essays</title>
  <style>
    body {
      font-family: Arial, Helvetica, sans-serif;
      line-height: 1.6;
      color: #1f2937;
      background-color: #ffffff;
      margin: 0;
      padding: 20px;
    }
    .container {
      max-width: 900px;
      margin: 0 auto;
      padding: 20px;
      border: 1px solid #e5e7eb;
      border-radius: 8px;
      background-color: #f9fafb;
    }
    h1, h2, h3 {
      color: #111827;
      margin-top: 0;
    }
    .paper-card {
      background: #ffffff;
      padding: 20px;
      border-radius: 8px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.1);
      margin-bottom: 24px;
    }
    .title {
      font-size: 1.25em;
      font-weight: bold;
      color: #1e40af;
      margin-bottom: 8px;
    }
    .author-list {
      font-size: 0.95em;
      color: #4b5563;
      margin-bottom: 8px;
    }
    .summary {
      margin-bottom: 16px;
    }
    .importance {
      padding: 12px;
      background-color: #eff6ff;
      border-left: 4px solid #3b82f6;
      font-size: 0.95em;
    }
    .ranking {
      font-weight: bold;
      color: #059669;
    }
    .note {
      margin-top: 20px;
      padding: 12px;
      background-color: #f3f4f6;
      border-left: 4px solid #9ca3af;
      font-size: 0.9em;
    }
    .authors-info {
      font-size: 0.9em;
      color: #6b7280;
      margin-top: 4px;
    }
  </style>
</head>
<body>
  <div class="container">
    <h1>Paper Analysis & Ranking</h1>
    <p>Below is an analysis and ranking of the provided paper. The evaluation considers the authors' impact and the significance of the subject matter, which is at the intersection of AI ethics, societal impact, and technology.</p>

    <div class="paper-card">
      <div class="title">
        <a href="https://arxiv.org/abs/2503.20062v2" target="_blank">Poor Alignment and Steerability of Large Language Models: Evidence from College Admission Essays</a>
      </div>
      <div class="author-list">
        Authors: Jinsook Lee, AJ Alvero, Thorsten Joachims, René Kizilcec
      </div>
      <div>
        <span class="authors-info">
          <em>Note: The author list is exceptionally strong, featuring prominent figures like <strong>Thorsten Joachims</strong> (a leading machine learning professor at Cornell, known for work on ranking, explainability, and fairness) and <strong>René Kizilcec</strong> (a leading researcher in human-computer interaction and the societal impacts of AI, especially in education at Cornell). This signals high credibility and expected impact.</em>
        </span>
      </div>

      <div class="summary">
        <strong>Summary:</strong> This study empirically investigates a critical, practical problem: the extent to which Large Language Models (LLMs) can write like humans (alignment) and whether they can be steered to mimic specific sociodemographic writing styles (e.g., based on sex, race, first-gen status). Using a novel, large-scale dataset of 30,000 real college admission essays, the authors compare their linguistic features (lexical and sentence variation) against LLM-generated essays (both unprompted and prompted with demographic information). Their key finding is that LLM essays are consistently linguistically distinct from human essays. Crucially, demographic prompting is ineffective for achieving steerability along these dimensions; it fails to align the model's output with human linguistic patterns for specific groups and actually exacerbates homogenization.
      </div>

      <div class="importance">
        <strong>Importance & Impact Assessment:</strong>
        <br>
        - <span class="ranking">Very High Impact</span>
        <br>
        - <strong>Subject Significance:</strong> The research directly addresses a core, timely concern about the use of LLMs in high-stakes decision-making contexts (e.g., admissions, hiring, legal documents). The failure of steerability has profound implications for fairness, equity, and the realistic limits of using these models to represent diverse human populations. It's a rigorous, evidence-based critique of a common application.
        <br>
        - <strong>Author Impact:</strong> With the caliber of authors (especially Joachims and Kizilcec), this paper carries significant weight. Their prior work in ML fairness and societal impact is highly influential, ensuring the methodology and conclusions are likely to be trusted and widely discussed in both technical and policy circles.
        <br>
        - <strong>Key Contributions:</strong> The paper's strength is its strong empirical grounding using a unique, high-stakes real-world dataset. It provides compelling evidence against the overestimation of LLM steerability and highlights a fundamental limitation in current LLMs. The finding about homogenization is particularly important for understanding bias. This work will likely shape the discourse on the responsible deployment of LLMs.
      </div>
    </div>

    <div class="note">
      <strong>Conclusion & Ranking:</strong> This is a <strong>very high impact</strong> paper with significant societal and ethical implications. Its rigorous methodology, choice of a high-stakes real-world context, and the clear, concerning findings about LLM steerability make it an essential read for anyone involved in AI ethics, policy, or the development of AI for sensitive applications. The impact is on par with the first paper, but with a different focus—societal application rather than technical security.
    </div>
  </div>
</body>
</html>